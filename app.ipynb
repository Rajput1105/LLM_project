{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat with LLaMA! Type 'exit' to stop.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LLaMA: [INST] hi  [/INST] hi  [/INST] hi  [/INST] hi  [/INST] hi  [/INST] hi  [/INST] hi  [/INST] hi  [/INST] hi  [/INST] hi  [/INST] hi  [/INST] hi  [/INST] hi  [/INST] hi  [/INST] hi  [/INST] hi  [/INST] hi  [/INST] hi  [/INST] hi  [/INST] hi  [/INST] hi  [/INST] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# import torch\n",
    "\n",
    "# # Load model and tokenizer\n",
    "# model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name, \n",
    "#     torch_dtype=torch.float32, \n",
    "#     device_map=\"cpu\"\n",
    "# )\n",
    "\n",
    "# # Interactive loop\n",
    "# while True:\n",
    "#     prompt = input(\"\\nYou: \")\n",
    "#     if prompt.lower() in [\"exit\", \"quit\"]:\n",
    "#         break\n",
    "\n",
    "#     inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cpu\")\n",
    "#     outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "\n",
    "#     response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "#     print(\"\\nLLaMA:\", response)\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Model Name\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "# Load Tokenizer and Model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float32)\n",
    "# Chat Function\n",
    "def chat():\n",
    "    print(\"Chat with LLaMA! Type 'exit' to stop.\\n\")\n",
    "    while True:\n",
    "        prompt = input(\"You: \")\n",
    "        if prompt.lower() in [\"exit\", \"quit\"]:\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        # Format the input as an instruction\n",
    "        formatted_prompt = f\"[INST] {prompt} [/INST]\"\n",
    "\n",
    "        inputs = tokenizer(formatted_prompt, return_tensors=\"pt\")\n",
    "        outputs = model.generate(\n",
    "            **inputs, \n",
    "            max_new_tokens=100,\n",
    "            temperature=0.7,  # Introduces randomness\n",
    "            top_k=50,         # Filters unlikely tokens\n",
    "            top_p=0.9         # Nucleus sampling\n",
    "        )\n",
    "\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        print(\"\\nLLaMA:\", response, \"\\n\")\n",
    "\n",
    "# Start Chat\n",
    "chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integration with LLamba for RAG based Retrival with chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collection and storing in markdown code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import html2text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_100='https://www.screener.in/screens/885655/top-100-stocks/'\n",
    "response=requests.get(url_100)\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup=BeautifulSoup(response.content,'html.parser')\n",
    "title=soup.find('h1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_content=soup.find('div',{'class':'responsive-holder fill-card-width'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_title=html2text.html2text(str(title))\n",
    "markdown_data=html2text.html2text(str(data_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./Data/100stocks.md\",\"w\") as f:\n",
    "    f.write(f\"TiTle:{markdown_title}\")\n",
    "    f.write(markdown_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb \n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "import json\n",
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger.add('app.log',level=\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model=SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "Data_path='./Data/100stocks.md'\n",
    "with open(Data_path,\"r\",encoding='utf-8') as f:\n",
    "    Data_100=f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding=embedding_model.encode(Data_100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chromadb client and collection setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Persistent client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client=chromadb.PersistentClient(path='./chroma_db')\n",
    "collection=client.create_collection(\"financial_report_of_top100_stocks\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using ChromaDB Server\n",
    "\n",
    "To use the server, the client should be running in the background:\n",
    "\n",
    "```bash\n",
    "chromadb run --path './chroma_db' --port 2000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clien_http=chromadb.HttpClient(host='localhost',port=2000)\n",
    "http_collection=client.get_collection('name of collection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-03-25 12:56:05.979\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m8\u001b[0m - \u001b[1mdata added succesfully\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "collection.add(\n",
    "    documents=[Data_100],\n",
    "    embeddings=[embedding.tolist()],\n",
    "    metadatas=[{\"file_name\": os.path.basename(Data_path), \"format\": \"markdown\"}],\n",
    "    ids=[\"financial_report_of_top100_stocks\"]\n",
    "\n",
    ")\n",
    "logger.info('data added succesfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rag Implementation and data retrivel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AIzaSyDjS9U8O46EEsdrnWlrnZHntUh4IvkagUU'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "GOOGLE_API=os.getenv('GOOGLE_API_KEY')\n",
    "# üîπ Set up Google Gemini API Key\n",
    "api_key = GOOGLE_API\n",
    "api_url = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro:generateContent\"\n",
    "api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 2 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Gemini's Answer: Nestl√© India shows some mixed trends based on the provided data:\n",
      "\n",
      "* **Strong Financials:**  A high market capitalization (‚Çπ215,281.84 Cr) indicates a large and established company.  Their P/E ratio of 68.59 suggests investors are willing to pay a premium for its earnings, possibly due to perceived stability and growth potential.  High ROCE (169.08%) signifies efficient capital utilization.\n",
      "* **Concerning Profit Growth:** While net profit (‚Çπ696.13 Cr) is substantial, the negative quarterly profit variation (-5.28%) raises a flag. This decline in profit growth year-over-year needs further investigation to understand if it's a temporary blip or a sign of a deeper issue.\n",
      "* **Modest Sales Growth:**  Sales growth (3.90%) is also quite modest, indicating a potential slowdown in top-line expansion. This, coupled with the declining profit growth, might point towards challenges in the current market environment.\n",
      "* **Stable Dividend:** A dividend yield of 0.77% offers some return to shareholders, although it's not exceptionally high.\n",
      "\n",
      "\n",
      "Overall, Nestl√© India appears to be a financially sound company with a high market valuation. However, the recent dip in profit growth and modest sales growth warrants attention. As a financial advisor, I would recommend looking into the underlying reasons for these trends before making any investment decisions. It would be crucial to compare these figures with industry benchmarks and Nestl√© India's historical performance to get a clearer picture.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def retrival(str:question):\n",
    "    emb_que=embedding_model.encode(question)\n",
    "    #query collection from the database \n",
    "    data_retrived=collection.query(query_embeddings=[emb_que.tolist()],n_results=2)\n",
    "    documents_ret=[doc[0] for doc in data_retrived[\"documents\"] if doc]\n",
    "    if documents_ret:\n",
    "        retrieved_text = \"\\n\\n\".join(documents_ret)  # Combine multiple docs for better context\n",
    "    else:\n",
    "        retrieved_text = \"No relevant data found in the database.\"\n",
    "\n",
    "    #prompt for llm\n",
    "    prompt=f\"\"\" You are a financial advisor how answer as per the data give :{retrieved_text}\n",
    "\n",
    "    \n",
    "    Question: {question}\n",
    "    Answer:\n",
    "    \"\"\"    \n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "    }\n",
    "    payload = {\n",
    "        \"contents\": [{\"parts\": [{\"text\": prompt}]}]\n",
    "    }\n",
    "    \n",
    "    response = requests.post(\n",
    "        f\"{api_url}?key={api_key}\",\n",
    "        headers=headers,\n",
    "        json=payload\n",
    "    )\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        try:\n",
    "            output_text = result[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
    "            return output_text\n",
    "        except KeyError:\n",
    "            return \"‚ö†Ô∏è Unexpected response format from Gemini.\"\n",
    "    else:\n",
    "        return f\"‚ùå API Error ({response.status_code}): {response.text}\"\n",
    "\n",
    "\n",
    "\n",
    "question = \"Tell about the trends of the Nestle India ?\"\n",
    "answer = retrival(question)\n",
    "print(\"ü§ñ Gemini's Answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import chromadb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_vis=chromadb.PersistentClient('./chroma_db')\n",
    "collection_vis=client_vis.get_collection(\"financial_docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_data=collection_vis.get(include=['documents', 'embeddings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vis_data\n",
    "embeddings = np.array(vis_data[\"embeddings\"])\n",
    "documents = np.array(vis_data[\"documents\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embeddings: (2, 384)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape of embeddings: {embeddings.shape}\")  # Should be (N, 384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
