{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat with LLaMA! Type 'exit' to stop.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LLaMA: [INST] hi  [/INST] hi  [/INST] hi  [/INST] hi  [/INST] hi  [/INST] hi  [/INST] hi  [/INST] hi  [/INST] hi  [/INST] hi  [/INST] hi  [/INST] hi  [/INST] hi  [/INST] hi  [/INST] hi  [/INST] hi  [/INST] hi  [/INST] hi  [/INST] hi  [/INST] hi  [/INST] hi  [/INST] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# import torch\n",
    "\n",
    "# # Load model and tokenizer\n",
    "# model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name, \n",
    "#     torch_dtype=torch.float32, \n",
    "#     device_map=\"cpu\"\n",
    "# )\n",
    "\n",
    "# # Interactive loop\n",
    "# while True:\n",
    "#     prompt = input(\"\\nYou: \")\n",
    "#     if prompt.lower() in [\"exit\", \"quit\"]:\n",
    "#         break\n",
    "\n",
    "#     inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cpu\")\n",
    "#     outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "\n",
    "#     response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "#     print(\"\\nLLaMA:\", response)\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Model Name\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "# Load Tokenizer and Model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float32)\n",
    "# Chat Function\n",
    "def chat():\n",
    "    print(\"Chat with LLaMA! Type 'exit' to stop.\\n\")\n",
    "    while True:\n",
    "        prompt = input(\"You: \")\n",
    "        if prompt.lower() in [\"exit\", \"quit\"]:\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        # Format the input as an instruction\n",
    "        formatted_prompt = f\"[INST] {prompt} [/INST]\"\n",
    "\n",
    "        inputs = tokenizer(formatted_prompt, return_tensors=\"pt\")\n",
    "        outputs = model.generate(\n",
    "            **inputs, \n",
    "            max_new_tokens=100,\n",
    "            temperature=0.7,  # Introduces randomness\n",
    "            top_k=50,         # Filters unlikely tokens\n",
    "            top_p=0.9         # Nucleus sampling\n",
    "        )\n",
    "\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        print(\"\\nLLaMA:\", response, \"\\n\")\n",
    "\n",
    "# Start Chat\n",
    "chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integration with LLamba for RAG based Retrival with chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collection and storing in markdown code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import html2text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_100='https://www.screener.in/screens/885655/top-100-stocks/'\n",
    "response=requests.get(url_100)\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup=BeautifulSoup(response.content,'html.parser')\n",
    "title=soup.find('h1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_content=soup.find('div',{'class':'responsive-holder fill-card-width'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_title=html2text.html2text(str(title))\n",
    "markdown_data=html2text.html2text(str(data_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./Data/100stocks.md\",\"w\") as f:\n",
    "    f.write(f\"TiTle:{markdown_title}\")\n",
    "    f.write(markdown_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb \n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "import json\n",
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger.add('app.log',level=\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model=SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "Data_path='./Data/100stocks.md'\n",
    "with open(Data_path,\"r\",encoding='utf-8') as f:\n",
    "    Data_100=f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding=embedding_model.encode(Data_100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chromadb client and collection setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Persistent client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client=chromadb.PersistentClient(path='./chroma_db')\n",
    "collection=client.create_collection(\"financial_report_of_top100_stocks\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using ChromaDB Server\n",
    "\n",
    "To use the server, the client should be running in the background:\n",
    "\n",
    "```bash\n",
    "chromadb run --path './chroma_db' --port 2000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clien_http=chromadb.HttpClient(host='localhost',port=2000)\n",
    "http_collection=client.get_collection('name of collection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-03-25 12:56:05.979\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m8\u001b[0m - \u001b[1mdata added succesfully\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "collection.add(\n",
    "    documents=[Data_100],\n",
    "    embeddings=[embedding.tolist()],\n",
    "    metadatas=[{\"file_name\": os.path.basename(Data_path), \"format\": \"markdown\"}],\n",
    "    ids=[\"financial_report_of_top100_stocks\"]\n",
    "\n",
    ")\n",
    "logger.info('data added succesfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rag Implementation and data retrivel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AIzaSyDjS9U8O46EEsdrnWlrnZHntUh4IvkagUU'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "GOOGLE_API=os.getenv('GOOGLE_API_KEY')\n",
    "# 🔹 Set up Google Gemini API Key\n",
    "api_key = GOOGLE_API\n",
    "api_url = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro:generateContent\"\n",
    "api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 2 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 Gemini's Answer: Nestlé India shows some mixed trends based on the provided data:\n",
      "\n",
      "* **Strong Financials:**  A high market capitalization (₹215,281.84 Cr) indicates a large and established company.  Their P/E ratio of 68.59 suggests investors are willing to pay a premium for its earnings, possibly due to perceived stability and growth potential.  High ROCE (169.08%) signifies efficient capital utilization.\n",
      "* **Concerning Profit Growth:** While net profit (₹696.13 Cr) is substantial, the negative quarterly profit variation (-5.28%) raises a flag. This decline in profit growth year-over-year needs further investigation to understand if it's a temporary blip or a sign of a deeper issue.\n",
      "* **Modest Sales Growth:**  Sales growth (3.90%) is also quite modest, indicating a potential slowdown in top-line expansion. This, coupled with the declining profit growth, might point towards challenges in the current market environment.\n",
      "* **Stable Dividend:** A dividend yield of 0.77% offers some return to shareholders, although it's not exceptionally high.\n",
      "\n",
      "\n",
      "Overall, Nestlé India appears to be a financially sound company with a high market valuation. However, the recent dip in profit growth and modest sales growth warrants attention. As a financial advisor, I would recommend looking into the underlying reasons for these trends before making any investment decisions. It would be crucial to compare these figures with industry benchmarks and Nestlé India's historical performance to get a clearer picture.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def retrival(str:question):\n",
    "    emb_que=embedding_model.encode(question)\n",
    "    #query collection from the database \n",
    "    data_retrived=collection.query(query_embeddings=[emb_que.tolist()],n_results=2)\n",
    "    documents_ret=[doc[0] for doc in data_retrived[\"documents\"] if doc]\n",
    "    if documents_ret:\n",
    "        retrieved_text = \"\\n\\n\".join(documents_ret)  # Combine multiple docs for better context\n",
    "    else:\n",
    "        retrieved_text = \"No relevant data found in the database.\"\n",
    "\n",
    "    #prompt for llm\n",
    "    prompt=f\"\"\" You are a financial advisor how answer as per the data give :{retrieved_text}\n",
    "\n",
    "    \n",
    "    Question: {question}\n",
    "    Answer:\n",
    "    \"\"\"    \n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "    }\n",
    "    payload = {\n",
    "        \"contents\": [{\"parts\": [{\"text\": prompt}]}]\n",
    "    }\n",
    "    \n",
    "    response = requests.post(\n",
    "        f\"{api_url}?key={api_key}\",\n",
    "        headers=headers,\n",
    "        json=payload\n",
    "    )\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        try:\n",
    "            output_text = result[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
    "            return output_text\n",
    "        except KeyError:\n",
    "            return \"⚠️ Unexpected response format from Gemini.\"\n",
    "    else:\n",
    "        return f\"❌ API Error ({response.status_code}): {response.text}\"\n",
    "\n",
    "\n",
    "\n",
    "question = \"Tell about the trends of the Nestle India ?\"\n",
    "answer = retrival(question)\n",
    "print(\"🤖 Gemini's Answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import chromadb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_vis=chromadb.PersistentClient('./chroma_db')\n",
    "collection_vis=client_vis.get_collection(\"financial_docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_data=collection_vis.get(include=['documents', 'embeddings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vis_data\n",
    "embeddings = np.array(vis_data[\"embeddings\"])\n",
    "documents = np.array(vis_data[\"documents\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embeddings: (2, 384)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape of embeddings: {embeddings.shape}\")  # Should be (N, 384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
