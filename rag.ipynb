{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG model with truera and LLambaIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! huggingface-cli login\n",
    "# ! pip install llama_index\n",
    "\n",
    "# if using colab or jupiter pip install ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## login with your hugging face credential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88d5963d2a954a12bc9e1ca16571f8a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: huggingface-cli <command> [<args>]\n",
      "huggingface-cli: error: argument {download,upload,repo-files,env,login,whoami,logout,auth,repo,lfs-enable-largefiles,lfs-multipart-upload,scan-cache,delete-cache,tag,version,upload-large-folder}: invalid choice: 'models' (choose from 'download', 'upload', 'repo-files', 'env', 'login', 'whoami', 'logout', 'auth', 'repo', 'lfs-enable-largefiles', 'lfs-multipart-upload', 'scan-cache', 'delete-cache', 'tag', 'version', 'upload-large-folder')\n"
     ]
    }
   ],
   "source": [
    "! huggingface-cli models --search \"llama\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu and disk.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Paris is the capital of France. Answer is 2 sentences : The capital of France is Paris. Answer is 3 sentences : The capital of France is Paris. Answer is 4 sentences : The capital of France is Paris. Answer is 5 sentences : The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.core.settings import Settings\n",
    "\n",
    "hf_llm = HuggingFaceLLM(\n",
    "    model_name=\"meta-llama/Llama-3.2-1B\",\n",
    "    tokenizer_name=\"meta-llama/Llama-3.2-1B\",\n",
    "    context_window=1024,  # Reduce from 2048\n",
    "    max_new_tokens=100,    # Reduce from 256\n",
    "    generate_kwargs={\"temperature\": 0.7, \"top_p\": 0.9},\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Use Settings instead of ServiceContext\n",
    "Settings.llm = hf_llm\n",
    "\n",
    "# Test the model\n",
    "response = hf_llm.complete(\"Answer is 1 sentence : What is the capital of France?\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: google-generativeai\n",
      "Version: 0.8.4\n",
      "Summary: Google Generative AI High level API client library and tools.\n",
      "Home-page: https://github.com/google/generative-ai-python\n",
      "Author: Google LLC\n",
      "Author-email: googleapis-packages@google.com\n",
      "License: Apache 2.0\n",
      "Location: /home/rahul-raj/LLM/.venv/lib/python3.10/site-packages\n",
      "Requires: google-ai-generativelanguage, google-api-core, google-api-python-client, google-auth, protobuf, pydantic, tqdm, typing-extensions\n",
      "Required-by: langchain-google-genai\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Or set them directly using below code \n",
    "# os.environ[\"GOOGLE_API_KEY\"] = \"your-api-key\"\n",
    "# genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load api from the .env file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "api_google=os.getenv('GOOGLE_API_KEY')\n",
    "api_google"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GEMINAI with custom LLM Wrapper with llama index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's impossible to say definitively which stock is the \"best\" for dividends in India, as market conditions and company performance constantly change.  Furthermore, providing specific investment advice is beyond my capabilities as an AI and against my ethical guidelines.  However, I can give you factors to consider and some general information about historically good dividend payers in India:\n",
      "\n",
      "**Factors to Consider When Evaluating Dividend Stocks:**\n",
      "\n",
      "* **Dividend Yield:** This is the annual dividend per share divided by the share price.  A higher yield *can* be attractive, but be cautious, as an unusually high yield can sometimes signal financial trouble.\n",
      "* **Dividend Payout Ratio:** This is the percentage of earnings paid out as dividends.  A sustainable payout ratio is generally considered to be below 70%.  A higher ratio might indicate that the company is struggling to reinvest in its growth.\n",
      "* **Company Performance:** Look for companies with a consistent history of profitability and dividend payments.  Past performance doesn't guarantee future results, but it can be a good indicator of stability.\n",
      "* **Debt Levels:** High levels of debt can make it difficult for a company to maintain dividend payments.\n",
      "* **Industry:** Some industries are known for being more stable and having higher dividend payouts than others.  Utilities, for example, often have steady dividend payments.\n",
      "* **Growth Prospects:**  While dividends are important, also consider the company's growth potential. A company with good growth prospects might offer lower dividends initially but could provide higher returns in the long run.\n",
      "\n",
      "**Historically, some Indian companies known for good dividend payouts include (but are not limited to):**\n",
      "\n",
      "* **Public Sector Companies (PSUs):**  Companies like Coal India, Power Grid Corporation, and Indian Oil Corporation have often offered good dividends.  However, their growth potential might be limited compared to private sector companies.\n",
      "* **Large-Cap Companies:**  Some large-cap companies in sectors like FMCG (Hindustan Unilever, ITC), IT (Infosys, TCS), and finance (HDFC Bank, ICICI Bank) have a history of paying dividends.\n",
      "* **Real Estate Investment Trusts (REITs) and Infrastructure Investment Trusts (InvITs):** These are designed to distribute a significant portion of their income as dividends.\n",
      "\n",
      "**Crucially:**\n",
      "\n",
      "* **Do your own research:**  Don't rely solely on past performance or recommendations.  Thoroughly research any company before investing.\n",
      "* **Consult with a financial advisor:** A qualified financial advisor can help you assess your investment goals and recommend suitable stocks based on your individual circumstances.\n",
      "* **Diversify your portfolio:** Don't put all your eggs in one basket.  Diversifying across different stocks and asset classes can help reduce risk.\n",
      "\n",
      "\n",
      "This information is for educational purposes only and is not financial advice.  Investing in the stock market involves risk, and you could lose money. Always consult with a qualified financial advisor before making any investment decisions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "from llama_index.core.llms import LLM\n",
    "from typing import Optional, Generator, Any\n",
    "from pydantic import Field, PrivateAttr\n",
    "\n",
    "\n",
    "# ✅ Set up Gemini API\n",
    "genai.configure(api_google)\n",
    "# ✅ Define Gemini LLM Wrapper with Pydantic fields\n",
    "class GeminiLLM(LLM):\n",
    "    model_name: str = Field(default=\"gemini-1.5-pro\", description=\"Gemini model name\")\n",
    "    temperature: float = Field(default=0.7, description=\"Temperature for generation\")\n",
    "\n",
    "    # ✅ Private attribute for non-Pydantic fields\n",
    "    _model: genai.GenerativeModel = PrivateAttr()\n",
    "\n",
    "    def __init__(self, model_name: str = \"gemini-1.5-pro\", temperature: float = 0.7):\n",
    "        super().__init__(model_name=model_name, temperature=temperature)\n",
    "        self._model = genai.GenerativeModel(model_name)  # ✅ Now stored as a private attribute\n",
    "\n",
    "    def complete(self, prompt: str) -> str:\n",
    "        \"\"\"Standard completion.\"\"\"\n",
    "        response = self._model.generate_content(prompt, generation_config={\"temperature\": self.temperature})\n",
    "        return response.text if response else \"No response\"\n",
    "\n",
    "    async def acomplete(self, prompt: str) -> str:\n",
    "        \"\"\"Async completion.\"\"\"\n",
    "        return self.complete(prompt)\n",
    "\n",
    "    def chat(self, messages: list[dict[str, str]]) -> str:\n",
    "        \"\"\"Chat-style conversation.\"\"\"\n",
    "        response = self._model.generate_content(messages[-1][\"content\"])\n",
    "        return response.text if response else \"No response\"\n",
    "\n",
    "    async def achat(self, messages: list[dict[str, str]]) -> str:\n",
    "        \"\"\"Async chat.\"\"\"\n",
    "        return self.chat(messages)\n",
    "\n",
    "    def stream_complete(self, prompt: str) -> Generator[str, None, None]:\n",
    "        \"\"\"Stream response in chunks.\"\"\"\n",
    "        for chunk in self._model.generate_content(prompt, stream=True):\n",
    "            yield chunk.text\n",
    "\n",
    "    async def astream_complete(self, prompt: str) -> Generator[str, None, None]:\n",
    "        \"\"\"Async streaming.\"\"\"\n",
    "        async for chunk in self.stream_complete(prompt):\n",
    "            yield chunk\n",
    "\n",
    "    def stream_chat(self, messages: list[dict[str, str]]) -> Generator[str, None, None]:\n",
    "        \"\"\"Stream chat responses.\"\"\"\n",
    "        for chunk in self._model.generate_content(messages[-1][\"content\"], stream=True):\n",
    "            yield chunk.text\n",
    "\n",
    "    async def astream_chat(self, messages: list[dict[str, str]]) -> Generator[str, None, None]:\n",
    "        \"\"\"Async chat streaming.\"\"\"\n",
    "        async for chunk in self.stream_chat(messages):\n",
    "            yield chunk\n",
    "\n",
    "    @property\n",
    "    def metadata(self) -> dict[str, Any]:\n",
    "        \"\"\"Model metadata.\"\"\"\n",
    "        return {\n",
    "            \"model_name\": self.model_name,\n",
    "            \"temperature\": self.temperature,\n",
    "        }\n",
    "\n",
    "# ✅ Instantiate and Test Gemini LLM\n",
    "gemini_llm = GeminiLLM()\n",
    "\n",
    "# Test Completion\n",
    "response = gemini_llm.complete(\"best stock to buy in india with good divedent ?\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
